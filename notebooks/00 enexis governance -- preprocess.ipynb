{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5068a6c-9915-4a56-a36f-72c6b6a500e4",
   "metadata": {},
   "source": [
    "# 00 - Preprocess the Governance Data Set\n",
    "This notebook explores and preprocesses the governence documents data set. We experiment with extraction from various forms of input document, word clouds to get a sense of word frequency and of course machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159afee-c514-48df-a646-ec5e7bc14e77",
   "metadata": {},
   "source": [
    "* Import CornÃ©'s python script\n",
    "* GM0007CA01.zip contains a link instead of the text\n",
    "* investigate empty files that do have text in the source\n",
    "* investigate oddly short files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28044c-c163-445d-ad3e-7d16ae3167d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Installation and Dependencies\n",
    "Here we install dependencies.\n",
    "\n",
    "For text extraction from the Word documents we use [Antiword](https://pypi.org/project/antiword/), a text extraction front-end to Libreoffice. Your Notebook server should have [Libreoffice](https://www.libreoffice.org/) installed.\n",
    "\n",
    "For PDF we use two-stage extraction: if the text extraction does not yield a significant document, we run Tesseract OCR on the documents to extract text from the images-as-pages form of PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1397ca88-6feb-4c38-bfb5-993e02c8538e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: pymupdf in /opt/conda/lib/python3.10/site-packages (1.22.3)\n",
      "Requirement already satisfied: filetype in /opt/conda/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: textract in /opt/conda/lib/python3.10/site-packages (1.6.5)\n",
      "Requirement already satisfied: antiword in /opt/conda/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.10/site-packages (1.9.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.8.2)\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.10/site-packages (1.3.6)\n",
      "Requirement already satisfied: pdf2image in /opt/conda/lib/python3.10/site-packages (1.16.3)\n",
      "Requirement already satisfied: pytesseract in /opt/conda/lib/python3.10/site-packages (0.3.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six~=1.12.0 in /opt/conda/lib/python3.10/site-packages (from textract) (1.12.0)\n",
      "Requirement already satisfied: python-pptx~=0.6.18 in /opt/conda/lib/python3.10/site-packages (from textract) (0.6.21)\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.10/site-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: extract-msg<=0.29.* in /opt/conda/lib/python3.10/site-packages (from textract) (0.28.7)\n",
      "Requirement already satisfied: argcomplete~=1.10.0 in /opt/conda/lib/python3.10/site-packages (from textract) (1.10.3)\n",
      "Requirement already satisfied: docx2txt~=0.8 in /opt/conda/lib/python3.10/site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: SpeechRecognition~=3.8.1 in /opt/conda/lib/python3.10/site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: xlrd~=1.2.0 in /opt/conda/lib/python3.10/site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20191110 in /opt/conda/lib/python3.10/site-packages (from textract) (20191110)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20191110->textract) (3.18.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (23.0)\n",
      "Requirement already satisfied: tzlocal>=2.1 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (5.0.1)\n",
      "Requirement already satisfied: olefile>=0.46 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
      "Requirement already satisfied: ebcdic>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
      "Requirement already satisfied: imapclient==2.1.0 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
      "Requirement already satisfied: compressed-rtf>=1.0.6 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (3.1.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.39.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pymupdf filetype textract antiword nltk wordcloud beautifulsoup4 unidecode pdf2image pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16e28df-da87-4dc6-b64a-81bfe5ac77aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python==3.10.9\n",
      "pymupdf==1.22.0\n",
      "filetype==1.2.0\n",
      "textract==1.6.5\n",
      "wordcloud==1.9.2\n",
      "matplotlib==3.7.1\n",
      "nltk==3.8.1\n",
      "beautifulsoup4==4.8.2\n",
      "chardet==3.0.4\n",
      "pandas==2.0.2\n",
      "pytesseract==0.3.10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "WRITE='w'\n",
    "READ_BINARY='rb'\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "print(\"python=={}\".format(re.sub(r'\\s.*', '', sys.version)))\n",
    "\n",
    "import fitz\n",
    "from fitz import VersionFitz as fitz__version__\n",
    "print(f\"pymupdf=={fitz__version__}\")\n",
    "\n",
    "import filetype\n",
    "print(f\"filetype=={filetype.__version__}\")\n",
    "\n",
    "import textract\n",
    "from textract import VERSION as textract__version__\n",
    "print(f\"textract=={textract__version__}\")\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import __version__ as wordcloud__version__\n",
    "print(f\"wordcloud=={wordcloud__version__}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import __version__ as matplotlib__version__\n",
    "print(f\"matplotlib=={matplotlib__version__}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(f\"nltk=={nltk.__version__}\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import __version__ as bs4__version__\n",
    "print(f\"beautifulsoup4=={bs4__version__}\")\n",
    "\n",
    "import chardet\n",
    "print(f\"chardet=={chardet.__version__}\")\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"pandas=={pd.__version__}\")\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image.exceptions import PDFPageCountError\n",
    "\n",
    "import pytesseract\n",
    "print(f\"pytesseract=={pytesseract.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cce7a0-920e-4a7a-93e5-97e372fe06b9",
   "metadata": {},
   "source": [
    "---\n",
    "## Structure of the Data\n",
    "As this is a project with unstructured data, we should preserve what little structure there is. The raw data comes as hand written documents from many authors, though the file names are meaningful. This section documents the data structure that we use on disk in this project and defines Python constants and a few [file name globs](https://realpython.com/get-all-files-in-directory-python/#using-a-python-glob-pattern-for-conditional-listing) to make document handling easier.\n",
    "\n",
    "### Directory Structure\n",
    "The raw data set is *not* in GIT, simply because of the file size. You can ask the project team for the original data set. The raw data files are stored in `executing-enexis/data/Governance/GM*.*` (known as `GLOB_RAW_DOCUMENTS` in Python code). Extracted text and document artifacts can be found under `executing-enexis/cache/Governance/` (`DATA_DIR` in Python).\n",
    "\n",
    "### File Name Structure\n",
    "The file names to the documents are fixed-length records with the following meaning: `\"GM\"<gemeente code (4-positions)><document type (2 positions)><id number (2 positions>\".\"<extension>`.\n",
    "\n",
    "The gemeente code can be mapped with the codes in `XXX`.\n",
    "\n",
    "The two-character document type can be found in the table below. Each document type has a glob for quick access to all documents of that type. These globs can be found in the same table, although you may find the mapping intuitive.\n",
    "\n",
    "| Document Type ID | Document Type             | Pathlib Glob   |\n",
    "|------------------|---------------------------|----------------\n",
    "| CA               | Coalitieakkoord           | `GLOB_CA`      |\n",
    "| DV               | Duurzaamheidsprogramma    | `GLOB_DV`      |\n",
    "| EX               | Externe documenten        | `GLOB_EX`      |\n",
    "| IK               | Inkoopbeleid              | `GLOB_IK`      |\n",
    "| JS               | Jaarstukken               | `GLOB_JS`      |\n",
    "| OB               | Overige beleidsdocumenten | `GLOB_OB`      |\n",
    "| PB               | Programmabegroting        | `GLOB_PB`      |\n",
    "| TV               | Toekomstvise              | `GLOB_TV`      |\n",
    "| WS               | Website gemeente          | `GLOB_WS`      |\n",
    "\n",
    "### File and Data Issues\n",
    "There are a number of issues with the data set, as is to be expected by a set produced by humans. The data set was constructed for a different purpose, where future text processing was not a requirement.\n",
    "\n",
    "* The file extension is not completely reliable. Especially documents with extension `.zip` seem not to be of mime type `application/zip`, but rather of type `text/html`. More on that in the text extraction, below.\n",
    "* Some of the Word documents only contain a URL to what seems to be a municipality website. The system used to collect the data and it seems that some people just created a document with the URL rather than uploading the actual document.\n",
    "* Some of the PDF documents have been produced with systems that write out the text as images. This makes for pretty formatting, but means we need OCR to extract the actual text from the document.\n",
    "\n",
    "There are a few more, insignificant oddities for which the code below has provisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fc5b64-cd37-42c0-8785-91e6a5337314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw documents: ../data/Governance/GM????????.*\n",
      "extracted documents = ../cache/Governance/GM????????.txt\n"
     ]
    }
   ],
   "source": [
    "# take any path and map it from the data directory to the cache directory.\n",
    "def to_cache(path):\n",
    "    return Path(str(path).replace('/data/', '/cache/'))\n",
    "\n",
    "DATA_DIR = '../data/Governance'\n",
    "CACHE_DIR = '../cache/Governance'\n",
    "Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The raw documents, warts and all; PDFs, ZIP files and Word documents.\n",
    "GLOB_RAW_DOCUMENTS = DATA_DIR + '/GM????????.*'\n",
    "\n",
    "# The files containing the extracted text from the raw documents.\n",
    "GLOB_ALL_DOCUMENTS = CACHE_DIR + '/GM????????.txt'\n",
    "\n",
    "GLOB_CA = CACHE_DIR + '/GM????CA??.txt'\n",
    "GLOB_DV = CACHE_DIR + '/GM????DV??.txt'\n",
    "GLOB_EX = CACHE_DIR + '/GM????EX??.txt'\n",
    "GLOB_IK = CACHE_DIR + '/GM????IK??.txt'\n",
    "GLOB_JS = CACHE_DIR + '/GM????JS??.txt'\n",
    "GLOB_OB = CACHE_DIR + '/GM????OB??.txt'\n",
    "GLOB_PB = CACHE_DIR + '/GM????PB??.txt'\n",
    "GLOB_TV = CACHE_DIR + '/GM????TV??.txt'\n",
    "GLOB_WS = CACHE_DIR + '/GM????WS??.txt'\n",
    "\n",
    "# take a glob and make it iterable. We cannot use globs as objects, since these get\n",
    "# \"exhausted\" when you iterate over them.\n",
    "# https://stackoverflow.com/questions/51108256/how-to-take-a-pathname-string-with-wildcards-and-resolve-the-glob-with-pathlib\n",
    "def expand_glob(glob):\n",
    "    p = Path(glob)\n",
    "    return Path(p.parent).expanduser().glob(p.name)\n",
    "\n",
    "print(f\"raw documents: {GLOB_RAW_DOCUMENTS}\")\n",
    "print(f\"extracted documents = {GLOB_ALL_DOCUMENTS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a553e-3c77-4570-a2ab-94ffc08dcb42",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Extraction\n",
    "This section extracts the text from the raw documents. It deals with the file formats and file encodings. Text extraction from Word files is done with Antiword and Libreoffice. For the PDFs we use [PyMuPDF](https://pypi.org/project/PyMuPDF/) instead. Antiword's development seems to have stagnated, so we only use it where we have to. HTML is cleaned up using [Beautiful Soup](https://pypi.org/project/beautifulsoup4/).\n",
    "\n",
    "As usual there are a few document oddities to contend with: a mix of file encodings, some documents are empty and some have the wrong [MIME type](https://en.wikipedia.org/wiki/Media_type#Mime.types). For example, documents with extension `.zip` seem not to be of MIME type `application/zip`, but rather of type `text/html`. Some of these `.zip` files are empty as well. There is one text document that contains CSS, so we exclude that specific file.\n",
    "\n",
    "For tokenization we chose quite a limited set of acceptable characters: only alphabetic characters and a few [accented characters](https://nl.wikipedia.org/wiki/Accenttekens_in_de_Nederlandse_spelling) that are common in the Dutch language. Early experiments showed that a lot of numerical \"words\" would creep into the data. That makes sense as many of these documents talk about finance, but hinders finding meaning. Numbers really only make sense in context and we simply don't have the context. By stripping out the numbers we also stripped the years out of the text, which do have meaning out of context. We chose to leave these out.\n",
    "\n",
    "The text extraction also performs some light preprocessing on the text, such as [stop word removal](https://en.wikipedia.org/wiki/Stop_word) and [stemming](https://en.wikipedia.org/wiki/Stemming), taking care to use the Dutch form of each of these. These are fairly slow operations, so they combine well with the extract-and-cache step of the process. That said, we should not preprocess too much, lest we subtly change the meaning of the text.\n",
    "\n",
    "Due to the nature, number and size of the raw documents, text extraction takes quite a bit of time. To speed up iterating over the data set we cache the resultant files in `CACHE_DIR` for quick access. These can then be accessed using the globs defined earlier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61dcdb40-e499-42e7-9cf1-7821cb28d800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# The stopwords for the Dutch language. These come as downloadable sets with NLTK.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "dutch_stopwords = set(stopwords.words('dutch'))\n",
    "\n",
    "# The stemming utility, also for the Dutch language.\n",
    "dutch_stemmer = SnowballStemmer('dutch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423f70bb-5c93-4e31-99aa-21ea5db063a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokens_of(text):\n",
    "    text = \" \".join(text.splitlines())  # remove line endings and make the file one, huge string,\n",
    "    text = unidecode(text)              # boil Unicode back to ASCII, removing accents,\n",
    "    text = text.lower().strip()         # lowercase everything\n",
    "                                        # and use only ascii, three+ letter sequences as tokens/words/terms\n",
    "    return re.split('[^a-zA-Z]+', text)\n",
    "\n",
    "def stemmed_tokens(text, stemmer):\n",
    "    return [stemmer.stem(token) for token in tokens_of(text)]\n",
    "\n",
    "def cleaned_and_stemmed_text(from_file, text, stopwords, stemmer):\n",
    "    text = ' '.join([token for token in stemmed_tokens(text, stemmer) if (token not in stopwords) and (len(token) > 2)])\n",
    "    if len(text) < 2048:\n",
    "        print(f\"suspiciously small file {from_file} has only {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "# we look for text in PDF's both as document text and by running OCR on the\n",
    "# pages. This way we are robust against documents that have been generated in\n",
    "# image format too.\n",
    "def extract_pdf(from_file, to_file, stopwords, stemmer):\n",
    "    text = \"\"\n",
    "    for page in fitz.open(from_file):\n",
    "        text = text + ' ' + page.get_text('text')\n",
    "    \n",
    "    # if the extracted text is tiny, we run OCR as well\n",
    "    if len(text) < 100:\n",
    "        try:\n",
    "            print(f\"{from_file} has only {len(text)} characters, trying OCR...\")\n",
    "            for page in convert_from_path(from_file.resolve()):\n",
    "                text = text + ' ' + pytesseract.image_to_string(page)\n",
    "        except PDFPageCountError as e:\n",
    "            print(f\"OCR failed on {from_file}: {str(e)}\")\n",
    "\n",
    "    with open(to_file, WRITE) as f:\n",
    "        text = cleaned_and_stemmed_text(from_file, text, stopwords, stemmer)\n",
    "        f.write(text)\n",
    "        f.write(' ')\n",
    "\n",
    "def extract_msword(from_file, to_file, stopwords, stemmer):\n",
    "    with open(to_file, WRITE) as f:\n",
    "        text = str(textract.process(str(from_file)))\n",
    "        text = cleaned_and_stemmed_text(from_file, text, stopwords, stemmer)\n",
    "        f.write(text)\n",
    "\n",
    "def encoding_of(file):\n",
    "    rawdata = open(file, READ_BINARY).read()\n",
    "    if len(rawdata) == 0:\n",
    "        return 'ascii'\n",
    "    encoding = chardet.detect(rawdata)['encoding']\n",
    "    print(f\"loading {file} using {encoding} encoding\")\n",
    "    return encoding\n",
    "\n",
    "def extract_plain(from_file, to_file, stopwords, stemmer):\n",
    "    text = from_file.read_text(encoding=encoding_of(from_file))\n",
    "    if len(text) == 0:\n",
    "        print(f\"ignoring {from_file} as it is empty\")\n",
    "    else:\n",
    "        with open(to_file, WRITE) as f:\n",
    "            text = cleaned_and_stemmed_text(from_file, text, stopwords, stemmer)\n",
    "            f.write(text)\n",
    "\n",
    "# https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python\n",
    "\n",
    "def extract_html(from_file, to_file, stopwords, stemmer):\n",
    "    text = from_file.read_text(encoding=encoding_of(from_file))\n",
    "    if len(text) == 0:\n",
    "        print(f\"ignoring {from_file} as it is empty\")\n",
    "    else:\n",
    "        with open(to_file, WRITE) as f:\n",
    "            soup = BeautifulSoup(text, features=\"html.parser\")\n",
    "            # r=ip out script and style elements as these would pollute the text\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            text = cleaned_and_stemmed_text(from_file, text, stopwords, stemmer)\n",
    "            f.write(text)\n",
    "\n",
    "def extract_file(from_file):\n",
    "    mime_type = filetype.guess_mime(from_file)\n",
    "    to_file = to_cache(from_file.with_suffix('.txt'))\n",
    "\n",
    "    if to_file.exists():\n",
    "        pass\n",
    "    elif from_file.stem == 'GM1711OB02':\n",
    "        print(f\"ignoring {from_file} as it is of MIME type text/css\")\n",
    "    elif mime_type == \"application/pdf\":\n",
    "        extract_pdf(from_file, to_file, dutch_stopwords, dutch_stemmer)\n",
    "    elif mime_type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" or \\\n",
    "         mime_type == \"application/msword\":\n",
    "        extract_msword(from_file, to_file, dutch_stopwords, dutch_stemmer)\n",
    "    elif not mime_type:\n",
    "        if from_file.suffix.lower() == '.txt':\n",
    "            extract_plain(from_file, to_file, dutch_stopwords, dutch_stemmer)\n",
    "        if from_file.suffix.lower() == '.zip':\n",
    "            # seems wrong, but it is correct for our data...\n",
    "            extract_html(from_file, to_file, dutch_stopwords, dutch_stemmer)\n",
    "    else:\n",
    "        raise Exception(f\"don't known how to handle {from_file} of MIME type {mime_type}\")\n",
    "\n",
    "# trying sequential first...\n",
    "def extract_dir(glob):\n",
    "     for from_file in expand_glob(glob):\n",
    "            extract_file(from_file)\n",
    "     #with Pool(multiprocessing.cpu_count() + 1) as p:\n",
    "     #   p.map(extract_file, [file for file in expand_glob(glob)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f3d0f-0d2a-43d0-a784-c708f7b89d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suspiciously small file ../data/Governance/GM0063JS01.pdf has only 0 characters\n",
      "suspiciously small file ../data/Governance/GM0147CA02.pdf has only 0 characters\n",
      "suspiciously small file ../data/Governance/GM0147CA03.pdf has only 0 characters\n",
      "ignoring ../data/Governance/GM0147PB02.zip as it is empty\n",
      "ignoring ../data/Governance/GM0225OB02.zip as it is empty\n",
      "suspiciously small file ../data/Governance/GM0277PB01.pdf has only 0 characters\n",
      "suspiciously small file ../data/Governance/GM0339JS01.pdf has only 0 characters\n",
      "suspiciously small file ../data/Governance/GM0393JS01.pdf has only 0 characters\n",
      "loading ../data/Governance/GM0399PB03.zip using ascii encoding\n",
      "suspiciously small file ../data/Governance/GM0399PB03.zip has only 1144 characters\n",
      "ignoring ../data/Governance/GM0439CA02.zip as it is empty\n",
      "ignoring ../data/Governance/GM0439CA03.zip as it is empty\n",
      "ignoring ../data/Governance/GM0542CA01.zip as it is empty\n",
      "suspiciously small file ../data/Governance/GM0569PB01.pdf has only 0 characters\n",
      "../data/Governance/GM0579IK01.pdf has only 16 characters, trying OCR...\n"
     ]
    }
   ],
   "source": [
    "extract_dir(GLOB_RAW_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c4b5b-cbf9-4fe4-a566-ff90a86ad2b2",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "The data set was assembled by a group of people answering a questionnaire and uploading document supporting their arguments in the process. Sometimes they would upload the same document to answer more than one question on the questionnaire, so we have a few duplicate files. Here we identify duplicates by checksum and remove their cached text documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7d055-8885-422f-b281-df58168c4e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file\n",
    "\n",
    "def md5(file):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file, READ_BINARY) as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def delete_duplicates(glob):\n",
    "    hashes = set()\n",
    "    dupes = 0\n",
    "    for file in expand_glob(glob):\n",
    "        hash = md5(file)\n",
    "        if hash in hashes:\n",
    "            print(f\"deleting {file} as dupe\")\n",
    "            file.unlink()\n",
    "            dupes += 1\n",
    "        else:\n",
    "            hashes.add(hash)\n",
    "    print(f\"deleted {dupes} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba6d71-e1a8-41c8-9759-54bf3914f8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_duplicates(GLOB_ALL_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c898af-87ed-4e93-9424-6504d65d36fd",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Word Clouds\n",
    "We now go through the text documents again, and generate a word cloud image for each file. There are too many to show word clouds for individual files, so we don't try. We do generate and show the word clouds for a few of the collections of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d535d22-d7c3-43f2-ab70-d9076f6df7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_one_file_cloud(file):\n",
    "    try:\n",
    "        cloud = WordCloud(background_color=\"white\", max_words=20).generate(file.read_text())\n",
    "        cloud.to_file(file.with_suffix('.png'))\n",
    "    except ValueError as e:\n",
    "        print(f\"cannot generate word cloud for {file}: {e}\")\n",
    "\n",
    "def make_per_file_cloud(glob):\n",
    "    with Pool(multiprocessing.cpu_count() + 1) as p:\n",
    "        p.map(make_one_file_cloud, [file for file in expand_glob(glob)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711551ba-3833-4538-bcd3-1d60fb34f27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_per_file_cloud(GLOB_ALL_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b27ec-f797-4855-8bf1-3f198d9c67ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_word_cloud(glob):\n",
    "    corpus = \"\"\n",
    "    for file in expand_glob(glob):\n",
    "        corpus = corpus + \" \" + file.read_text()\n",
    "\n",
    "    cloud = WordCloud(background_color=\"white\", max_words=50).generate(corpus)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fd2da-d58b-4c03-89a0-e56b60433f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_ALL_DOCUMENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f75ece-eaab-44b7-b9d4-432e4171af17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_CA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a0845-a5c0-4a51-bb8e-42f6ed81d57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_DV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a048b22-7ed6-49fc-9bc9-581d255d1a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_EX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cf350-ab94-4ca1-ba45-0c1118a76372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_IK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484a73e-40d7-4db8-84b9-2a3af36d2499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_JS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd271f1-1fbe-4da0-bac6-1d3c231dc60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_OB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3aec7-ff0d-4258-8583-b11a8793bff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_PB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea860e-d000-47f0-98ad-8daeee41b0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_TV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5b30d-1ea4-461c-8c02-057d99028795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_WS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb1cca-b5fd-4438-822d-90007a21688e",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Type and File Type Counts\n",
    "Here we count and graph the doucment types and file types, just to add some colour to the document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb9980-6f22-45bf-98c2-c71e1e4c5f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_dir(glob):\n",
    "    mime_types = []\n",
    "    extensions = []\n",
    "    docu_types = []\n",
    "\n",
    "    for from_file in expand_glob(glob):\n",
    "        mime_type = filetype.guess_mime(from_file)\n",
    "        if from_file.stem == 'GM1711OB02':\n",
    "            mime_types.append(\"text/css\")\n",
    "        elif not mime_type:\n",
    "            if from_file.suffix.lower() == '.txt':\n",
    "                mime_types.append(\"text/plain\")\n",
    "            elif from_file.suffix.lower() == '.zip':\n",
    "                # seems wrong, but it is correct for our data...\n",
    "                mime_types.append(\"text/html\")\n",
    "            else:\n",
    "                raise Exception(f\"don't known how to handle {from_file} of MIME type {mime_type}\")\n",
    "        else:\n",
    "            mime_types.append(mime_type)\n",
    "            \n",
    "        extensions.append(from_file.suffix.lower())\n",
    "        docu_types.append(from_file.stem[6:8])\n",
    "\n",
    "    return pd.DataFrame.from_records(list(dict(Counter(mime_types)).items())).set_index(0), \\\n",
    "           pd.DataFrame.from_records(list(dict(Counter(extensions)).items())).set_index(0), \\\n",
    "           pd.DataFrame.from_records(list(dict(Counter(docu_types)).items())).set_index(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bafa2-6394-4fcb-873e-1c02cdd90272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mime_types, extensions, docu_types = count_dir(GLOB_RAW_DOCUMENTS)\n",
    "\n",
    "mime_types.sort_values(by=1).plot.barh();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6845d-e170-4a08-a579-3427239fcd4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docu_types.sort_values(by=1).plot.barh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216cdfb-b00f-49c8-b2e4-d7676052c770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extensions.sort_values(by=1).plot.barh();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35401203-216f-4f06-9536-1b5268cf5197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893f9fe-7e5c-4147-ba17-98e32789cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX Histogram documenten formaat\n",
    "XXX Beslissen waar we de knip leggen voor documenten die te klein zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359bddf-dff8-4514-9b07-7e6a776286aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382dcb-e13a-4466-bade-b5e370e64b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
