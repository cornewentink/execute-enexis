{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0890ba85-abc0-4a6d-9d16-5ab213f3f9b1",
   "metadata": {},
   "source": [
    "# 01 - TF-IDF on the Governance Set\n",
    "This notebook runs TF-IDF on the governance data set.\n",
    "\n",
    "The code in this notebook was roughly based on [TF-IDF Vectorizer scikit-learn](https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a) by Mukesh Chaudhary. We replicate their steps and build from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3367b64-b505-4c11-9cf2-f4d612d3f22a",
   "metadata": {},
   "source": [
    "**braindump**\n",
    "\n",
    "* XXX histogram woorden per klasse van document frequency maken voor CornÃ©. Iets van 20 bins. Document frequency uitrekenen. Doel: df_min en df_max bestuderen.\n",
    "* histogram df met aantal woorden binnen de DV set, zodat je kunt zien wat df_min en df_max doen.\n",
    "* scree plot om aantal topics te bepalen\n",
    "    - https://stackoverflow.com/questions/69091520/determine-the-correct-number-of-topics-using-latent-semantic-analysis\n",
    "* commonality value @Mariek\n",
    "* daar een beperkte set topics maken en dan per combinatie een scatterplot van de woord gewichten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a95aae-68a2-4a0f-a0f8-b9c054f45a3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Dependencies and Imports\n",
    "This is where we install and import the dependencies. Most operations are performed using Scikit-learn ans Panda's. We also load Wordcloud and Matplotlib for visualising the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8290fa-e5f5-43d9-b670-fa22044b10ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.11/site-packages (1.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263bd0dd-6e06-48a1-9b2d-090dd09e53d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python==3.11.4\n",
      "scikit-learn==1.2.2\n",
      "pandas==2.0.2\n",
      "numpy==1.25.0\n",
      "wordcloud==1.9.2\n",
      "matplotlib==3.7.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "WRITE='w'\n",
    "READ_BINARY='rb'\n",
    "print(\"python=={}\".format(re.sub(r'\\s.*', '', sys.version)))\n",
    "\n",
    "from sklearn import __version__ as sklearn__version__\n",
    "print(f\"scikit-learn=={sklearn__version__}\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"pandas=={pd.__version__}\")\n",
    "ROW    = 0\n",
    "COLUMN = 1\n",
    "STRING = 'string'\n",
    "OBJECT = 'object'\n",
    "NUMBER = 'number'\n",
    "CATEGORY = 'category'\n",
    "INTEGER = 'integer'\n",
    "UNSIGNED = 'unsigned'\n",
    "FLOAT = 'float'\n",
    "\n",
    "import numpy as np\n",
    "print(f\"numpy=={np.__version__}\")\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import __version__ as wordcloud__version__\n",
    "print(f\"wordcloud=={wordcloud__version__}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import __version__ as matplotlib__version__\n",
    "print(f\"matplotlib=={matplotlib__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b3ece-9636-4328-9573-e2d6877a39c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading\n",
    "All data was preprocessed by the \"_00 - Preprocess the Governance Data Set_\" notebook, so our data loading steps here can be simplified. We don't have to worry about tokenization, stemming and stop words. For this notebook we really only need the DV data set and for comparison we load the complete corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c60927-1ab6-4814-8edb-8f9a82313c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = '../cache/Governance'\n",
    "\n",
    "# The Parquet files, gzipped.\n",
    "ALL_PARQUET_GZ = CACHE_DIR + '/ALL_documents.parquet.gz'\n",
    "DV_PARQUET_GZ  = CACHE_DIR + '/DV_documents.parquet.gz'\n",
    "\n",
    "ALL_corpus = pd.read_parquet(ALL_PARQUET_GZ)\n",
    "DV_corpus  = pd.read_parquet(DV_PARQUET_GZ)\n",
    "\n",
    "# columns ofthe data set\n",
    "DOCUMENT_BODY = 'body'\n",
    "DOCUMENT_TITLE = 'Titel'\n",
    "DOCUMENT_JAAR = 'Jaar'\n",
    "MUNICIPALITY_CODE='GM_CODE'\n",
    "\n",
    "DV_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce131cf-21a5-453d-9376-e139b58e0d76",
   "metadata": {},
   "source": [
    "For use in graphs, the Enexis and Google Sheets colour codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b3e01-15f7-4027-8939-78eaa3db5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENEXIS_PINK='#cc2b72'\n",
    "ENEXIS_DARK_PINK='#a72a81'\n",
    "ENEXIS_VERY_DARK_PINK='#942d88'\n",
    "ENEXIS_GREEN='#c3da45'\n",
    "ENEXIS_DARKGREEN='#94b950'\n",
    "ENEXIS_LIGHTGREY='#f0f0f0'\n",
    "\n",
    "SHEET_GREEN='#00b3ae'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25276d-5384-45c1-9c9e-c3dba072dda7",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Frequency Tuning\n",
    "The document frequency turned out to be a great way to clip off common (for the corpus) words from the data sets. More on that below. For now, we perform our TF-IDF analysis using the best values we found.\n",
    "\n",
    "As our documents are quite large, we set `sublinear_tf` to apply a $log()$ to the TF. This reduces the significance of document size on the output.\n",
    "\n",
    "*note*: the data type of `min_df` and `max_df` changes its meaning. `int`s are taken to mean counts, while `float`s are taken to mean percentages. We mix the two here, as you can see in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82334317-ecd0-4fd9-8857-9ae51d9801a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DF = 15   # count\n",
    "MAX_DF = 0.85 # percent\n",
    "SUBLINEAR_TF = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ce7fb-dcf6-4cfb-92ee-c3e737bbb56d",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply Vectorizers to Governance Data Set\n",
    "Here we define and run TF-IDF on the governance data sets. The count vectorizer is a good way to get some idea of how the documents and words relate. A better understanding of that helps with `max_df` and `min_df` tuning, for example.\n",
    "\n",
    "The main reason to define these functions is to ensure the operations yield clean data frames for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2a606-3637-4188-aafc-2c5a6a576ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_vectorize(series):\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # run the vectorizer on the data\n",
    "    word_matrix = vectorizer.fit_transform(series)\n",
    "    words_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # take the output and package it into various useful data frames\n",
    "    per_document    = pd.DataFrame(index=series.index, columns=words_list, data=word_matrix.toarray())\n",
    "    sum_over_corpus = pd.DataFrame(per_document.sum(), columns=['sum']).T\n",
    "\n",
    "    return vectorizer, per_document, sum_over_corpus\n",
    "\n",
    "\n",
    "def tfidf_vectorize(series, min_df, max_df, sublinear_tf):\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, sublinear_tf=sublinear_tf)\n",
    "\n",
    "    # run the vectorizer on the data\n",
    "    word_matrix = vectorizer.fit_transform(series)\n",
    "    words_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # take the output and package it into various useful data frames\n",
    "    matrix = pd.DataFrame(index=series.index, columns=words_list, data=word_matrix.toarray())\n",
    "    idf = pd.DataFrame(columns=words_list, data=[vectorizer.idf_])\n",
    "\n",
    "    return vectorizer, matrix, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a264b28-fcd4-4eb7-bc67-c952a6a31a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_docs_vectorizer, all_docs_matrix, all_docs_idf = tfidf_vectorize(ALL_corpus[DOCUMENT_BODY], min_df=MIN_DF, max_df=MAX_DF, sublinear_tf=SUBLINEAR_TF)\n",
    "all_docs_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10725c8-b7a9-43a9-8ff9-a0a94378f9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#all_docs_vectorizer.stop_words_\n",
    "len(all_docs_vectorizer.stop_words_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b867680-dfde-4148-8053-9ad0c31054ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dv_docs_vectorizer, dv_docs_matrix, dv_docs_idf = tfidf_vectorize(DV_corpus[DOCUMENT_BODY], min_df=MIN_DF, max_df=MAX_DF, sublinear_tf=SUBLINEAR_TF)\n",
    "dv_docs_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d6581-5eac-47dd-be46-46444868493a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dv_docs_vectorizer.stop_words_\n",
    "len(dv_docs_vectorizer.stop_words_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104272f-c5af-4eee-9a1e-dfa36e1813a3",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction via `min_df` and `max_df` Tuning\n",
    "In order to determine useful values for `min_df` and `max_df`. For that we need a histogram of the document frequency of each term.\n",
    "\n",
    "In the dimensionality reduction we strive to remove words that are either not part of many documents (these are probably not on topic and likely to be spelling errors or specific to a certain municipality), or that are part of almost all documents. These do not contribute to the clustering and just end up making the clusters look very similar.\n",
    "\n",
    "We arrived at the values for `MIN_DF` and `MAX_DF` by experimenting with a few word clouds, until we saw that the weasel words disappeared and the clusters started making sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab9d39-61b1-4a2a-90e6-7f4cae286d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "DV_document_count = DV_corpus[DOCUMENT_BODY].shape[0]\n",
    "max_df_line = int(DV_document_count * MAX_DF)\n",
    "\n",
    "_, count_per_dv_document, _ = count_vectorize(DV_corpus[DOCUMENT_BODY])\n",
    "\n",
    "histo_data = count_per_dv_document.astype(bool).sum(axis=ROW).sort_values()\n",
    "histo_data[histo_data<MIN_DF].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65021b-3e45-41b8-b69e-eca3bf459af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_data[histo_data>max_df_line].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece95b5-d742-4253-85e4-e77addadda47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"min_df={MIN_DF} and max_df={max_df_line} ({MAX_DF*100}% of {DV_document_count} documents)\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "ax.axvline(x=MIN_DF, color=ENEXIS_PINK, label=f\"min_df={MIN_DF}\")\n",
    "ax.axvline(x=max_df_line, color=ENEXIS_PINK, label=f\"max_df={max_df_line} ({MAX_DF*100}% of {DV_document_count} documents)\")\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.spines[['left', 'bottom']].set_color(ENEXIS_LIGHTGREY)\n",
    "\n",
    "# plt.legend(loc='upper right', fontsize=9)\n",
    "plt.hist(histo_data, bins=DV_document_count, color=SHEET_GREEN);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33296dbe-e4f5-47fa-9974-61561d680e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_per_dv_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018811c-3fef-47c8-8ff8-9bccac7d8618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_DV = dv_docs_matrix.mean().dropna().sort_values()\n",
    "total_DV.nlargest(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae6fd3-16f2-4493-b7cb-8e4fa9084088",
   "metadata": {},
   "source": [
    "---\n",
    "## Word Cloud of Unique Words in DV vs All\n",
    "Here we subtract the two word matrixes \"all\" and \"dv\" to determine what words are identifying for DV documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91740fca-880e-4b00-85a7-2654396fdf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_for_DV = (dv_docs_matrix.mean() - all_docs_matrix.mean()).dropna().sort_values()\n",
    "unique_for_DV.nlargest(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd903c4e-debc-441f-82dd-3ac17c595c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(unique_for_DV)\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd01755-d89b-4a99-afec-38a046fd6559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_on_idf = (dv_docs_idf.T - all_docs_idf.T).dropna()\n",
    "unique_on_idf[0].nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f36c2-c984-441d-ac26-869019c25fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(unique_on_idf[0])\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c1b3d-a9ae-4e7f-ad0f-91479986e1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dv_docs_idf.T[0].nlargest(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4dfa6-7624-47da-bc86-33df9e045e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
