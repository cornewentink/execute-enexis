{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5068a6c-9915-4a56-a36f-72c6b6a500e4",
   "metadata": {},
   "source": [
    "# Explore the Governance Data Set\n",
    "\n",
    "This notebook explores the Governence documents data set. We experiment with extraction from various forms of input document, word clouds to get a sense of word frequency and of course machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159afee-c514-48df-a646-ec5e7bc14e77",
   "metadata": {},
   "source": [
    "* Import Corné's python script\n",
    "* GM0007CA01.zip contains a link instead of the text\n",
    "* invetigate empty files that do have text in the source\n",
    "* investigate oddly short files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28044c-c163-445d-ad3e-7d16ae3167d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Installation and Dependencies\n",
    "Here we install dependencies.\n",
    "\n",
    "For text extraction from the Word documents we use [Antiword](https://pypi.org/project/antiword/), a text extraction front-end to Libreoffice. Your Notebook server should have [Libreoffice](https://www.libreoffice.org/) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1397ca88-6feb-4c38-bfb5-993e02c8538e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /opt/conda/lib/python3.10/site-packages (1.22.3)\n",
      "Requirement already satisfied: filetype in /opt/conda/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: textract in /opt/conda/lib/python3.10/site-packages (1.6.5)\n",
      "Requirement already satisfied: antiword in /opt/conda/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.10/site-packages (1.9.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.8.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-pptx~=0.6.18 in /opt/conda/lib/python3.10/site-packages (from textract) (0.6.21)\n",
      "Requirement already satisfied: six~=1.12.0 in /opt/conda/lib/python3.10/site-packages (from textract) (1.12.0)\n",
      "Requirement already satisfied: extract-msg<=0.29.* in /opt/conda/lib/python3.10/site-packages (from textract) (0.28.7)\n",
      "Requirement already satisfied: docx2txt~=0.8 in /opt/conda/lib/python3.10/site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: argcomplete~=1.10.0 in /opt/conda/lib/python3.10/site-packages (from textract) (1.10.3)\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.10/site-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: xlrd~=1.2.0 in /opt/conda/lib/python3.10/site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20191110 in /opt/conda/lib/python3.10/site-packages (from textract) (20191110)\n",
      "Requirement already satisfied: SpeechRecognition~=3.8.1 in /opt/conda/lib/python3.10/site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20191110->textract) (3.17)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.0.1)\n",
      "Requirement already satisfied: tzlocal>=2.1 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (5.0.1)\n",
      "Requirement already satisfied: imapclient==2.1.0 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
      "Requirement already satisfied: compressed-rtf>=1.0.6 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
      "Requirement already satisfied: ebcdic>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
      "Requirement already satisfied: olefile>=0.46 in /opt/conda/lib/python3.10/site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.39.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (3.1.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf filetype textract antiword wordcloud beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f16e28df-da87-4dc6-b64a-81bfe5ac77aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python==3.10.9\n",
      "pymupdf==1.22.0\n",
      "filetype==1.2.0\n",
      "textract==1.6.5\n",
      "wordcloud==1.9.2\n",
      "matplotlib==3.7.1\n",
      "nltk==3.8.1\n",
      "beautifulsoup4==4.8.2\n",
      "chardet==3.0.4\n",
      "seaborn==0.12.2\n",
      "pandas==2.0.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "WRITE='w'\n",
    "READ_BINARY='rb'\n",
    "from collections import Counter\n",
    "print(\"python=={}\".format(re.sub(r'\\s.*', '', sys.version)))\n",
    "\n",
    "import fitz\n",
    "from fitz import VersionFitz as fitz__version__\n",
    "print(f\"pymupdf=={fitz__version__}\")\n",
    "\n",
    "import filetype\n",
    "print(f\"filetype=={filetype.__version__}\")\n",
    "\n",
    "import textract\n",
    "from textract import VERSION as textract__version__\n",
    "print(f\"textract=={textract__version__}\")\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import __version__ as wordcloud__version__\n",
    "print(f\"wordcloud=={wordcloud__version__}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import __version__ as matplotlib__version__\n",
    "print(f\"matplotlib=={matplotlib__version__}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(f\"nltk=={nltk.__version__}\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import __version__ as bs4__version__\n",
    "print(f\"beautifulsoup4=={bs4__version__}\")\n",
    "\n",
    "import chardet\n",
    "print(f\"chardet=={chardet.__version__}\")\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"pandas=={pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cce7a0-920e-4a7a-93e5-97e372fe06b9",
   "metadata": {},
   "source": [
    "---\n",
    "## Structure of the Data\n",
    "As this is a project with unstructured data, we should preserve what little structure there is. The raw data comes as hand written documents from many authors, though the file names are meaningful. This section documents the data structure that we use on disk in this project and defines Python constants and a few [file name globs](https://realpython.com/get-all-files-in-directory-python/#using-a-python-glob-pattern-for-conditional-listing) to make document handling easier.\n",
    "\n",
    "### Directory Structure\n",
    "The raw data set is *not* in GIT, simply because of the file size. You can ask the project team for the original data set. The raw data files are stored in `executing-enexis/data/Governance/GM*.*` (known as `GLOB_RAW_DOCUMENTS` in Python code). Extracted text and document artifacts can be found under `executing-enexis/cache/Governance/` (`DATA_DIR` in Python).\n",
    "\n",
    "### File Name Structure\n",
    "The file names to the documents are fixed-length records with the following meaning: `\"GM\"<gemeente code (4-positions)><document type (2 positions)><id number (2 positions>\".\"<extension>`.\n",
    "\n",
    "The gemeente code can be mapped with the codes in `XXX`.\n",
    "\n",
    "The two-character document type can be found in the table below. Each document type has a glob for quick access to all documents of that type. These globs can be found in the same table, although you may find the mapping intuitive.\n",
    "\n",
    "| Document Type ID | Document Type             | Pathlib Glob   |\n",
    "|------------------|---------------------------|----------------\n",
    "| CA               | Coalitieakkoord           | `GLOB_CA`      |\n",
    "| DV               | Duurzaamheidsprogramma    | `GLOB_DV`      |\n",
    "| EX               | Externe documenten        | `GLOB_EX`      |\n",
    "| IK               | Inkoopbeleid              | `GLOB_IK`      |\n",
    "| JS               | Jaarstukken               | `GLOB_JS`      |\n",
    "| OB               | Overige beleidsdocumenten | `GLOB_OB`      |\n",
    "| PB               | Programmabegroting        | `GLOB_PB`      |\n",
    "| TV               | Toekomstvise              | `GLOB_TV`      |\n",
    "| WS               | Website gemeente          | `GLOB_WS`      |\n",
    "\n",
    "The file extension is not completely reliable. Especially documents with extension `.zip` seem not to be of mime type `application/zip`, but rather of type `text/html`. More on that in the text extraction, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1fc5b64-cd37-42c0-8785-91e6a5337314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw documents: ../data/Governance/GM????????.*\n",
      "extracted documents = ../cache/Governance/GM????????.txt\n"
     ]
    }
   ],
   "source": [
    "# take any path and map it from the data directory to the cache directory.\n",
    "def to_cache(path):\n",
    "    return Path(str(path).replace('/data/', '/cache/'))\n",
    "\n",
    "DATA_DIR = '../data/Governance'\n",
    "CACHE_DIR = '../cache/Governance'\n",
    "Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The raw documents, warts and all; PDFs, ZIP files and Word documents.\n",
    "GLOB_RAW_DOCUMENTS = DATA_DIR + '/GM????????.*'\n",
    "\n",
    "# The files containing the extracted text from the raw documents.\n",
    "GLOB_ALL_DOCUMENTS = CACHE_DIR + '/GM????????.txt'\n",
    "\n",
    "GLOB_CA = CACHE_DIR + '/GM????CA??.txt'\n",
    "GLOB_DV = CACHE_DIR + '/GM????DV??.txt'\n",
    "GLOB_EX = CACHE_DIR + '/GM????EX??.txt'\n",
    "GLOB_IK = CACHE_DIR + '/GM????IK??.txt'\n",
    "GLOB_JS = CACHE_DIR + '/GM????JS??.txt'\n",
    "GLOB_OB = CACHE_DIR + '/GM????OB??.txt'\n",
    "GLOB_PB = CACHE_DIR + '/GM????PB??.txt'\n",
    "GLOB_TV = CACHE_DIR + '/GM????TV??.txt'\n",
    "GLOB_WS = CACHE_DIR + '/GM????WS??.txt'\n",
    "\n",
    "# take a glob and make it iterable. We cannot use globs as objects, since these get\n",
    "# \"exhausted\" when you iterate over them.\n",
    "# https://stackoverflow.com/questions/51108256/how-to-take-a-pathname-string-with-wildcards-and-resolve-the-glob-with-pathlib\n",
    "def expand_glob(glob):\n",
    "    p = Path(glob)\n",
    "    return Path(p.parent).expanduser().glob(p.name)\n",
    "\n",
    "print(f\"raw documents: {GLOB_RAW_DOCUMENTS}\")\n",
    "print(f\"extracted documents = {GLOB_ALL_DOCUMENTS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a553e-3c77-4570-a2ab-94ffc08dcb42",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Extraction\n",
    "This section extracts the text from the raw documents. It deals with the file formats and file encodings. Text extraction from Word files is done with Antiword and Libreoffice. For the PDFs we use [PyMuPDF](https://pypi.org/project/PyMuPDF/) instead. Antiword's development seems to have stagnated, so we only use it where we have to. HTML is cleaned up using [Beautiful Soup](https://pypi.org/project/beautifulsoup4/).\n",
    "\n",
    "As usual there are a few document oddities to contend with: a mix of file encodings, some documents are empty and some have the wrong [MIME type](https://en.wikipedia.org/wiki/Media_type#Mime.types). For example, documents with extension `.zip` seem not to be of MIME type `application/zip`, but rather of type `text/html`. Some of these `.zip` files are empty as well. There is one text document that contains CSS, so we exclude that specific file.\n",
    "\n",
    "For tokenization we chose quite a limited set of acceptable characters: only alphabetic characters and a few [accented characters](https://nl.wikipedia.org/wiki/Accenttekens_in_de_Nederlandse_spelling) that are common in the Dutch language. Early experiments showed that a lot of numerical \"words\" would creep into the data. That makes sense as many of these documents talk about finance, but hinders finding meaning. Numbers really only make sense in context and we simply don't have the context. By stripping out the numbers we also stripped the years out of the text, which do have meaning out of context. We chose to leave these out.\n",
    "\n",
    "The text extraction also performs some light preprocessing on the text, such as [stop word removal](https://en.wikipedia.org/wiki/Stop_word) and [stemming](https://en.wikipedia.org/wiki/Stemming), taking care to use the Dutch form of each of these. These are fairly slow operations, so they combine well with the extract-and-cache step of the process. That said, we should not preprocess too much, lest we subtly change the meaning of the text.\n",
    "\n",
    "Due to the nature, number and size of the raw documents, text extraction takes quite a bit of time. To speed up iterating over the data set we cache the resultant files in `CACHE_DIR` for quick access. These can then be accessed using the globs defined earlier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "61dcdb40-e499-42e7-9cf1-7821cb28d800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# The stopwords for the Dutch language. These come as downloadable sets with NLTK.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "dutch_stopwords = set(stopwords.words('dutch'))\n",
    "\n",
    "# The stemming utility, also for the Dutch language.\n",
    "dutch_stemmer = SnowballStemmer('dutch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "423f70bb-5c93-4e31-99aa-21ea5db063a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# XXX https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\n",
    "\n",
    "def stemmed_tokens(text, stemmer):\n",
    "    return [stemmer.stem(token.strip().lower()) for token in re.split('[^a-zA-Zëéèïöóà]+', text)]\n",
    "\n",
    "def cleaned_and_stemmed_text(text, stopwords, stemmer):\n",
    "    return ' '.join([token for token in stemmed_tokens(text, stemmer) if (token not in stopwords) and (len(token) > 2)])\n",
    "\n",
    "def extract_pdf(from_file, to_file, stopwords, stemmer):\n",
    "    document = fitz.open(from_file)\n",
    "    with open(to_file, WRITE) as f:\n",
    "        for page in document:\n",
    "            text = page.get_text('text')\n",
    "            text = cleaned_and_stemmed_text(text, stopwords, stemmer)\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n",
    "def extract_msword(from_file, to_file, stopwords, stemmer):\n",
    "    with open(to_file, WRITE) as f:\n",
    "        text = str(textract.process(str(from_file)))\n",
    "        text = cleaned_and_stemmed_text(text, stopwords, stemmer)\n",
    "        f.write(text)\n",
    "\n",
    "def encoding_of(file):\n",
    "    rawdata = open(file, \"rb\").read()\n",
    "    if len(rawdata) == 0:\n",
    "        return 'ascii'\n",
    "    encoding = chardet.detect(rawdata)['encoding']\n",
    "    print(f\"loading {file} using {encoding} encoding\")\n",
    "    return encoding\n",
    "\n",
    "def extract_plain(from_file, to_file, stopwords, stemmer):\n",
    "    text = from_file.read_text(encoding=encoding_of(from_file))\n",
    "    if len(text) == 0:\n",
    "        print(f\"ignoring {from_file} as it is empty\")\n",
    "    else:\n",
    "        with open(to_file, WRITE) as f:\n",
    "            text = cleaned_and_stemmed_text(text, stopwords, stemmer)\n",
    "            f.write(text)\n",
    "\n",
    "# https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python\n",
    "\n",
    "def extract_html(from_file, to_file, stopwords, stemmer):\n",
    "    text = from_file.read_text(encoding=encoding_of(from_file))\n",
    "    if len(text) == 0:\n",
    "        print(f\"ignoring {from_file} as it is empty\")\n",
    "    else:\n",
    "        with open(to_file, WRITE) as f:\n",
    "            soup = BeautifulSoup(text, features=\"html.parser\")\n",
    "            # r=ip out script and style elements as these would pollute the text\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            text = cleaned_and_stemmed_text(text, stopwords, stemmer)\n",
    "            f.write(text)\n",
    "\n",
    "def extract_dir(glob, stopwords, stemmer):\n",
    "    skipped = 0\n",
    "    for from_file in expand_glob(glob):\n",
    "        mime_type = filetype.guess_mime(from_file)\n",
    "        to_file = to_cache(from_file.with_suffix('.txt'))\n",
    "\n",
    "        if to_file.exists():\n",
    "            skipped += 1\n",
    "        elif from_file.stem == 'GM1711OB02':\n",
    "            print(f\"ignoring {from_file} as it is of MIME type text/css\")\n",
    "        elif mime_type == \"application/pdf\":\n",
    "            extract_pdf(from_file, to_file, stopwords, stemmer)\n",
    "        elif mime_type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" or \\\n",
    "             mime_type == \"application/msword\":\n",
    "            extract_msword(from_file, to_file, stopwords, stemmer)\n",
    "        elif not mime_type:\n",
    "            if from_file.suffix.lower() == '.txt':\n",
    "                extract_plain(from_file, to_file, stopwords, stemmer)\n",
    "            if from_file.suffix.lower() == '.zip':\n",
    "                # seems wrong, but it is correct for our data...\n",
    "                extract_html(from_file, to_file, stopwords, stemmer)\n",
    "        else:\n",
    "            raise Exception(f\"don't known how to handle {from_file} of MIME type {mime_type}\")\n",
    "    print(f\"skipped {skipped} files that have previously been extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "094f3d0f-0d2a-43d0-a784-c708f7b89d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring ../data/Governance/GM0147PB02.zip as it is empty\n",
      "ignoring ../data/Governance/GM0225OB02.zip as it is empty\n",
      "loading ../data/Governance/GM0399PB03.zip using ascii encoding\n",
      "ignoring ../data/Governance/GM0439CA02.zip as it is empty\n",
      "ignoring ../data/Governance/GM0439CA03.zip as it is empty\n",
      "ignoring ../data/Governance/GM0542CA01.zip as it is empty\n",
      "ignoring ../data/Governance/GM0613OB02.zip as it is empty\n",
      "ignoring ../data/Governance/GM0772IK02.zip as it is empty\n",
      "ignoring ../data/Governance/GM1680DV01.zip as it is empty\n",
      "ignoring ../data/Governance/GM1711OB02.txt as it is of MIME type text/css\n",
      "ignoring ../data/Governance/GM1916TV02.zip as it is empty\n",
      "skipped 2276 files that have previously been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_dir(GLOB_RAW_DOCUMENTS, dutch_stopwords, dutch_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c4b5b-cbf9-4fe4-a566-ff90a86ad2b2",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "The data set was assembled by a group of people answering a questionnaire and uploading document supporting their arguments in the process. Sometimes they would upload the same document to answer more than one question on the questionnaire, so we have a few duplicate files. Here we identify duplicates by checksum and remove their cached text documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9fe7d055-8885-422f-b281-df58168c4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file\n",
    "\n",
    "def md5(file):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file, READ_BINARY) as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def delete_duplicates(glob):\n",
    "    hashes = set()\n",
    "    dupes = 0\n",
    "    for file in expand_glob(glob):\n",
    "        hash = md5(file)\n",
    "        if hash in hashes:\n",
    "            print(f\"deleting {file} as dupe\")\n",
    "            file.unlink()\n",
    "            dupes += 1\n",
    "        else:\n",
    "            hashes.add(hash)\n",
    "    print(f\"deleted {dupes} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79ba6d71-e1a8-41c8-9759-54bf3914f8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting ../cache/Governance/GM0059IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0063CA02.txt as dupe\n",
      "deleting ../cache/Governance/GM0063CA03.txt as dupe\n",
      "deleting ../cache/Governance/GM0079EX01.txt as dupe\n",
      "deleting ../cache/Governance/GM0079IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0086CA02.txt as dupe\n",
      "deleting ../cache/Governance/GM0093DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0147CA03.txt as dupe\n",
      "deleting ../cache/Governance/GM0148DV02.txt as dupe\n",
      "deleting ../cache/Governance/GM0148OB02.txt as dupe\n",
      "deleting ../cache/Governance/GM0168DV02.txt as dupe\n",
      "deleting ../cache/Governance/GM0173IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0225CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0225JS01.txt as dupe\n",
      "deleting ../cache/Governance/GM0226IK02.txt as dupe\n",
      "deleting ../cache/Governance/GM0232CA02.txt as dupe\n",
      "deleting ../cache/Governance/GM0246IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0267PB02.txt as dupe\n",
      "deleting ../cache/Governance/GM0277OB03.txt as dupe\n",
      "deleting ../cache/Governance/GM0293IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0294CA02.txt as dupe\n",
      "deleting ../cache/Governance/GM0297OB01.txt as dupe\n",
      "deleting ../cache/Governance/GM0302OB01.txt as dupe\n",
      "deleting ../cache/Governance/GM0302TV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0310CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0327DV02.txt as dupe\n",
      "deleting ../cache/Governance/GM0327DV03.txt as dupe\n",
      "deleting ../cache/Governance/GM0353CA03.txt as dupe\n",
      "deleting ../cache/Governance/GM0361DV02.txt as dupe\n",
      "deleting ../cache/Governance/GM0361TV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0362IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0375CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0399PB03.txt as dupe\n",
      "deleting ../cache/Governance/GM0402IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0406IK02.txt as dupe\n",
      "deleting ../cache/Governance/GM0420IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0420OB03.txt as dupe\n",
      "deleting ../cache/Governance/GM0432CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0441DV02.txt as dupe\n",
      "deleting ../cache/Governance/GM0451CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0498IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0503CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0505IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0531IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0532PB02.txt as dupe\n",
      "deleting ../cache/Governance/GM0545OB02.txt as dupe\n",
      "deleting ../cache/Governance/GM0545TV02.txt as dupe\n",
      "deleting ../cache/Governance/GM0546IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0576EX01.txt as dupe\n",
      "deleting ../cache/Governance/GM0579CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0579IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0590IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0606OB02.txt as dupe\n",
      "deleting ../cache/Governance/GM0610IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0611WS01.txt as dupe\n",
      "deleting ../cache/Governance/GM0613CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0638IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0642IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0715IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0737DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0737IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0743CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0753IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0762CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0785CA02.txt as dupe\n",
      "deleting ../cache/Governance/GM0785CA03.txt as dupe\n",
      "deleting ../cache/Governance/GM0798DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0815PB02.txt as dupe\n",
      "deleting ../cache/Governance/GM0873TV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0879JS01.txt as dupe\n",
      "deleting ../cache/Governance/GM0984DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM0988IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM0994CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM0994OB01.txt as dupe\n",
      "deleting ../cache/Governance/GM0994TV02.txt as dupe\n",
      "deleting ../cache/Governance/GM1525OB01.txt as dupe\n",
      "deleting ../cache/Governance/GM1621CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM1640DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM1655PB01.txt as dupe\n",
      "deleting ../cache/Governance/GM1655TV01.txt as dupe\n",
      "deleting ../cache/Governance/GM1659CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM1663WS02.txt as dupe\n",
      "deleting ../cache/Governance/GM1669OB03.txt as dupe\n",
      "deleting ../cache/Governance/GM1680IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM1684IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM1684TV01.txt as dupe\n",
      "deleting ../cache/Governance/GM1702IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM1711CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM1722DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM1722IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM1722OB02.txt as dupe\n",
      "deleting ../cache/Governance/GM1730PB02.txt as dupe\n",
      "deleting ../cache/Governance/GM1884OB06.txt as dupe\n",
      "deleting ../cache/Governance/GM1891DV01.txt as dupe\n",
      "deleting ../cache/Governance/GM1891IK01.txt as dupe\n",
      "deleting ../cache/Governance/GM1891OB01.txt as dupe\n",
      "deleting ../cache/Governance/GM1891TV01.txt as dupe\n",
      "deleting ../cache/Governance/GM1904PB01.txt as dupe\n",
      "deleting ../cache/Governance/GM1927CA01.txt as dupe\n",
      "deleting ../cache/Governance/GM1987CA02.txt as dupe\n",
      "deleted 100 duplicates\n"
     ]
    }
   ],
   "source": [
    "delete_duplicates(GLOB_ALL_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c898af-87ed-4e93-9424-6504d65d36fd",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Word Clouds\n",
    "We now go through the text documents again, and generate a word cloud image for each file. There are too many to show word clouds for individual files, so we don't try. We do generate and show the word clouds for a few of the collections of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d535d22-d7c3-43f2-ab70-d9076f6df7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_per_file_cloud(glob):\n",
    "    for file in expand_glob(glob):\n",
    "        try:\n",
    "            cloud = WordCloud(background_color=\"white\", max_words=20).generate(file.read_text())\n",
    "            cloud.to_file(file.with_suffix('.png'))\n",
    "        except ValueError as e:\n",
    "            print(f\"cannot generate word cloud for {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711551ba-3833-4538-bcd3-1d60fb34f27f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot generate word cloud for ../cache/Governance/GM0005CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0014OB04.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0056CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0063JS01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0085IK01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0140IK01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0147CA02.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0148CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0160DV01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0180OB04.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0214CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0252OB01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0275CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0277PB01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0321OB02.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0327CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0339JS01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0353DV02.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0370CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0373DV01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0393JS01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0399CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0417CA01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0441DV01.txt: We need at least 1 word to plot a word cloud, got 0.\n",
      "cannot generate word cloud for ../cache/Governance/GM0448IK01.txt: We need at least 1 word to plot a word cloud, got 0.\n"
     ]
    }
   ],
   "source": [
    "make_per_file_cloud(GLOB_ALL_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b27ec-f797-4855-8bf1-3f198d9c67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_cloud(glob):\n",
    "    corpus = \"\"\n",
    "    for file in expand_glob(glob):\n",
    "        corpus = corpus + \" \" + file.read_text()\n",
    "\n",
    "    cloud = WordCloud(background_color=\"white\", max_words=50).generate(corpus)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fd2da-d58b-4c03-89a0-e56b60433f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_ALL_DOCUMENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f75ece-eaab-44b7-b9d4-432e4171af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_CA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a0845-a5c0-4a51-bb8e-42f6ed81d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_DV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a048b22-7ed6-49fc-9bc9-581d255d1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_EX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cf350-ab94-4ca1-ba45-0c1118a76372",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_IK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484a73e-40d7-4db8-84b9-2a3af36d2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_JS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd271f1-1fbe-4da0-bac6-1d3c231dc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_OB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3aec7-ff0d-4258-8583-b11a8793bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_PB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea860e-d000-47f0-98ad-8daeee41b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_TV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5b30d-1ea4-461c-8c02-057d99028795",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(GLOB_WS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb1cca-b5fd-4438-822d-90007a21688e",
   "metadata": {},
   "source": [
    "---\n",
    "## Document Type and File Type Counts\n",
    "Here we count and graph the doucment types and file types, just to add some colour to the document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb9980-6f22-45bf-98c2-c71e1e4c5f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_dir(glob):\n",
    "    mime_types = []\n",
    "    extensions = []\n",
    "    docu_types = []\n",
    "\n",
    "    for from_file in expand_glob(glob):\n",
    "        mime_type = filetype.guess_mime(from_file)\n",
    "        if from_file.stem == 'GM1711OB02':\n",
    "            mime_types.append(\"text/css\")\n",
    "        elif not mime_type:\n",
    "            if from_file.suffix.lower() == '.txt':\n",
    "                mime_types.append(\"text/plain\")\n",
    "            elif from_file.suffix.lower() == '.zip':\n",
    "                # seems wrong, but it is correct for our data...\n",
    "                mime_types.append(\"text/html\")\n",
    "            else:\n",
    "                raise Exception(f\"don't known how to handle {from_file} of MIME type {mime_type}\")\n",
    "        else:\n",
    "            mime_types.append(mime_type)\n",
    "            \n",
    "        extensions.append(from_file.suffix.lower())\n",
    "        docu_types.append(from_file.stem[6:8])\n",
    "\n",
    "    return pd.DataFrame.from_records(list(dict(Counter(mime_types)).items())).set_index(0), \\\n",
    "           pd.DataFrame.from_records(list(dict(Counter(extensions)).items())).set_index(0), \\\n",
    "           pd.DataFrame.from_records(list(dict(Counter(docu_types)).items())).set_index(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bafa2-6394-4fcb-873e-1c02cdd90272",
   "metadata": {},
   "outputs": [],
   "source": [
    "mime_types, extensions, docu_types = count_dir(GLOB_RAW_DOCUMENTS)\n",
    "\n",
    "mime_types.sort_values(by=1).plot.barh();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6845d-e170-4a08-a579-3427239fcd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_types.sort_values(by=1).plot.barh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216cdfb-b00f-49c8-b2e4-d7676052c770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extensions.sort_values(by=1).plot.barh();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35401203-216f-4f06-9536-1b5268cf5197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893f9fe-7e5c-4147-ba17-98e32789cea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359bddf-dff8-4514-9b07-7e6a776286aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382dcb-e13a-4466-bade-b5e370e64b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
